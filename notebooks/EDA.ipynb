{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "This notebook performs exploratory data analysis on stock price data to understand its statistical properties, identify patterns, and prepare for LSTM model training.\n",
    "## Objective:\n",
    "\n",
    "- Validate statistical properties of price and returns\n",
    "- Inspect volatility & trend regimes\n",
    "- Detect potential data leakage\n",
    "- Support LSTM + walk-forward trading design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for data analysis and visualization\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "# Add project root to Python path to enable imports from src module\n",
    "project_root = os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\")))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization style for consistent and clean plots\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "# Configure pandas to display floating point numbers with 6 decimal places\n",
    "pd.set_option(\"display.float_format\", \"{:.6f}\".format)\n",
    "\n",
    "# Import data loading utility from the project\n",
    "from src.data.loader import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load historical stock data for AAPL (Apple Inc.) from January 1, 2022 onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stock ticker and date range for analysis\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = None  # None means use the latest available date\n",
    "\n",
    "# Load the stock data using the custom data loader\n",
    "df = load_data(ticker, start_date, end_date)\n",
    "\n",
    "# Display first few rows to verify data was loaded correctly\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection\n",
    "\n",
    "Examine the structure, data types, and basic statistics of the loaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data types, column info, and memory usage\n",
    "# This helps identify any missing values or incorrect data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display statistical summary of all numeric columns\n",
    "# Includes count, mean, std, min, max, and quartiles\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Validation\n",
    "\n",
    "Perform assertions to ensure data meets quality requirements for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data quality with assertions:\n",
    "\n",
    "# 1. Check that index is DatetimeIndex for time series analysis\n",
    "assert isinstance(df.index, pd.DatetimeIndex)\n",
    "\n",
    "# 2. Ensure dates are in chronological order (no future dates mixed in)\n",
    "assert df.index.is_monotonic_increasing\n",
    "\n",
    "# 3. Verify there are no missing values in the dataset\n",
    "assert df.isnull().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Price Visualization\n",
    "\n",
    "Plot the historical closing prices to visualize the overall price movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with specified size for better visibility\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "# Plot the Close price column over time\n",
    "df[\"Close\"].plot(title=f\"{ticker} Closing Price\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Returns Calculation\n",
    "\n",
    "Calculate logarithmic returns, which are preferred over simple returns for statistical analysis due to their time-additivity and better statistical properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate log returns: log(P_t / P_{t-1})\n",
    "# Log returns are preferred because:\n",
    "# - They are time-additive (can sum across periods)\n",
    "# - They have better statistical properties (more normal-like)\n",
    "# - They are symmetric (gains and losses are treated equally)\n",
    "df[\"log_ret\"] = np.log(df[\"Close\"]).diff()\n",
    "\n",
    "# Remove the first row (NaN from diff operation)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Returns Statistics\n",
    "\n",
    "Examine the statistical distribution of log returns, including extreme percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed statistics of log returns including extreme percentiles\n",
    "# This helps identify the range of typical vs extreme returns\n",
    "df[\"log_ret\"].describe(percentiles=[0.01, 0.05, 0.95, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Returns Distribution\n",
    "\n",
    "Visualize the distribution of log returns to check for normality and identify outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram to visualize the distribution of log returns\n",
    "# 100 bins provide fine granularity to see the shape\n",
    "plt.figure(figsize=(10,4))\n",
    "df[\"log_ret\"].hist(bins=100)\n",
    "plt.title(\"Log Return Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation Analysis\n",
    "\n",
    "Plot the autocorrelation function (ACF) to check for temporal dependencies in returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ACF plotting function from statsmodels\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# Plot autocorrelation function for lag 1 to 30\n",
    "# This helps identify if past returns predict future returns\n",
    "plot_acf(df[\"log_ret\"], lags=60)\n",
    "plt.title(\"Autocorrelation of Log Returns\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volatility Calculation\n",
    "\n",
    "Calculate rolling volatility measures to identify volatility regimes and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 20-day rolling standard deviation of returns (short-term volatility)\n",
    "df[\"vol_20\"] = df[\"log_ret\"].rolling(20).std()\n",
    "\n",
    "# Calculate 60-day median of the 20-day rolling volatility (long-term volatility baseline)\n",
    "# Using median is more robust to outliers than mean\n",
    "df[\"vol_60_med\"] = df[\"vol_20\"].rolling(60).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volatility Regimes Visualization\n",
    "\n",
    "Plot the short-term and long-term volatility to identify different market regimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot both volatility measures over time to visualize regimes\n",
    "plt.figure(figsize=(12,4))\n",
    "df[\"vol_20\"].plot(label=\"20D Vol\")  # Short-term volatility\n",
    "df[\"vol_60_med\"].plot(label=\"60D Median Vol\")  # Long-term median volatility\n",
    "plt.legend()\n",
    "plt.title(\"Volatility Regimes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volatility Ratio Analysis\n",
    "\n",
    "Calculate the ratio of short-term to long-term volatility to identify regime changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate volatility ratio: short-term / long-term\n",
    "# Ratio > 1 indicates increased volatility (high volatility regime)\n",
    "# Ratio < 1 indicates decreased volatility (low volatility regime)\n",
    "vol_ratio = df[\"vol_20\"] / df[\"vol_60_med\"]\n",
    "vol_ratio.plot(title=\"Volatility Ratio (20D Vol / 60D Median Vol)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volatility Ratio Distribution\n",
    "\n",
    "Visualize the distribution of volatility ratios to set thresholds for regime classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of volatility ratio to see distribution\n",
    "plt.figure(figsize=(10,4))\n",
    "vol_ratio.hist(bins=50)\n",
    "\n",
    "# Add vertical line at 1.2 to mark potential high-volatility threshold\n",
    "plt.axvline(1.2, color=\"red\", linestyle=\"--\")\n",
    "plt.title(\"Volatility Ratio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend Strength Calculation\n",
    "\n",
    "Calculate a measure of trend strength based on price deviation from moving average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 50-day simple moving average of closing prices\n",
    "close = df[\"Close\"].iloc[:, 0]\n",
    "df[\"ma50\"] = close.rolling(50).mean()\n",
    "\n",
    "# Calculate trend strength as absolute percentage deviation from MA\n",
    "# Higher values indicate stronger trending behavior\n",
    "df[\"trend_strength\"] = (close - df[\"ma50\"]).abs() / df[\"ma50\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend Strength Time Series\n",
    "\n",
    "Visualize how trend strength evolves over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trend strength over time to see when strong trends occur\n",
    "plt.figure(figsize=(12,4))\n",
    "df[\"trend_strength\"].plot(title=\"Trend Strength\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend Strength Distribution\n",
    "\n",
    "Visualize the distribution of trend strength to determine threshold for trend identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of trend strength to identify typical vs extreme values\n",
    "plt.figure(figsize=(10,4))\n",
    "df[\"trend_strength\"].hist(bins=50)\n",
    "\n",
    "# Add vertical line at 0.0075 as potential threshold for strong trends\n",
    "plt.axvline(0.0075, color=\"red\", linestyle=\"--\")\n",
    "plt.title(\"Trend Strength Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regime Filtering Definition\n",
    "\n",
    "Define a regime filter based on volatility and trend strength to identify favorable trading conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regime_ok based on two conditions:\n",
    "# 1. Short-term volatility is less than 1.2x long-term median (not in high volatility regime)\n",
    "# 2. Trend strength is greater than 0.75% (strong enough trend to trade)\n",
    "df[\"regime_ok\"] = (\n",
    "    (df[\"vol_20\"] < 1.2 * df[\"vol_60_med\"]) &\n",
    "    (df[\"trend_strength\"] > 0.0075)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regime Distribution\n",
    "\n",
    "Examine the proportion of time spent in each regime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the distribution of regime_ok values as percentages\n",
    "# This tells us what fraction of time is suitable for trading\n",
    "df[\"regime_ok\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Correlation Analysis\n",
    "\n",
    "Examine correlations between different features to understand their relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for correlation analysis\n",
    "features = [\"log_ret\", \"vol_20\", \"trend_strength\"]\n",
    "\n",
    "# Display correlation matrix between selected features\n",
    "# This helps identify multicollinearity and feature relationships\n",
    "df[features].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Correlation with Future Returns\n",
    "\n",
    "Calculate how well each feature correlates with next-day returns to assess predictive potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target variable: next day's return (what we want to predict)\n",
    "df[\"future_ret\"] = df[\"log_ret\"].shift(-1)\n",
    "\n",
    "# Calculate correlation between features and future returns\n",
    "# This helps assess the predictive power of each feature\n",
    "df[features].corrwith(df[\"future_ret\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "Summary of findings from the exploratory data analysis:\n",
    "\n",
    "- Returns are noisy with near-zero autocorrelation\n",
    "- Volatility clustering is present\n",
    "- Trend regimes exist but are intermittent\n",
    "- Regime filtering is justified\n",
    "- Forecasting alone is weak â†’ filtering & sizing are critical"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
